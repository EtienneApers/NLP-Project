{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Homework.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "DmxSEb9-wAov",
        "2_tQqT2BwAo-"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eh1Q4afqwAog"
      },
      "source": [
        "# Using only URLs for unsupervized topic modelling\n",
        "Mathias Andler and Etienne Apers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANGeZ5XPwAoo"
      },
      "source": [
        "\n",
        "## Initial assessment given in March\n",
        " With the rise of internet and our connected world, we have access to an increasingly large number of news source from all around the globe. At the same time it is often said that we tend to only read news that confirm our own opinion. Taking steps to mitigate this is bias aren't necessarly easy when we often only read a few lines from an article before switching our attention, or when we only read a title. Our hypothesis is that by using only urls linking to a news article we would be able to situate politically the news source. \n",
        "\n",
        "We will use: Millions of News Article URLs: 2.3 million URLs for news articles from the frontpage of over 950 English-language news outlets in the six month period between October 2014 and April 2015.\n",
        "\n",
        "We will use Fastext vector in English as well as Word2vec because we will analyse the headlines at a word \n",
        "level.\n",
        "\n",
        "The objective is to build a Sequence Classification based on Sentiment analysis. The context and sentiment of the headlines will help us identify the political views of the newspapers in question.\n",
        "\n",
        "## Comments on this initial assessment and the actual work we did\n",
        "\n",
        "We didn't realize how \"dirty\" urls could be or that this was in fact an unsupervized task: there are no labels in this dataset\n",
        " <br /> \n",
        "We first had to go through a lenghty pre-processing to access the information within urls. Then we did word level vectorization. \n",
        " <br /> \n",
        " After that we shifted our work towards automatic topic detection, which can be done in an unsupervized way. We mainly used k-means methods. This was pretty sucessfull as can be seen in the notebook.\n",
        " <br /> \n",
        "Finally we attempted to solve our initial problem of situating politically news sources by using the topic detection tools we developped.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqP7WWHTalsw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ad6TZkHVwAop"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import gzip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHLe-UnYwAoq"
      },
      "source": [
        "# 1 Loading our data and pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgbAxSnvuxA7",
        "outputId": "b09fc96d-09f6-45de-a05f-09f7af24257e"
      },
      "source": [
        "! wget 'https://datadryad.org/stash/downloads/download_resource/1703' -O 'article-urls.zip'\n",
        "\n",
        "with ZipFile('article-urls.zip', 'r') as zp:\n",
        "    zp.printdir()\n",
        "    zp.extractall()\n",
        "\n",
        "tar = tarfile.open(\"article-urls.tar.gz\", \"r:gz\")\n",
        "tar.extractall()\n",
        "tar.close()\n",
        "\n",
        "os.rename('article-urls.txt','article-urls.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-23 15:52:55--  https://datadryad.org/stash/downloads/download_resource/1703\n",
            "Resolving datadryad.org (datadryad.org)... 54.186.186.103, 34.211.168.133, 44.228.64.246\n",
            "Connecting to datadryad.org (datadryad.org)|54.186.186.103|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc3-s3mrt1001-prd.s3.us-west-2.amazonaws.com/e4507710-8648-4024-99da-d2cd82fc2b94/data?response-content-disposition=attachment%3B%20filename%3Ddoi_10.5061_dryad.p8s0j__v1.zip&response-content-type=application%2Fzip&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEG8aCXVzLXdlc3QtMiJGMEQCIFbd%2BOCWHKBgjxS2z%2B87%2B3zDAdUUp1KUZEX9AOeeYtkdAiBSbYhuCyETTNoMGthEFHSybLmGmK5W2b2IOXZHhMS7Iyq9AwjY%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAAaDDQ1MTgyNjkxNDE1NyIMXm6%2FikkAO1now%2F1LKpED1MoRR7EvItVKSWE6KSOuLz9857PUOrxRkMBVvOdLedTIg1xn%2F8dPmkbGV%2FMHDd4UI%2FWyz6US6FRNueExqSlMgXxpV4oAWeaU4YYcnn6dOYUdEAR9j3nWgCXX4tkWrkZJYqPrj9EX%2FiISmwSubUpQa4Af721iLnree7MxYtG8iUoESvkGUkbiQlwv7wQcbVZkgqZOy3M3tit%2F5sorLgCJflxoxj%2FeJK4RLLWTxLE2ZN02QyJCT%2F5bq0IARErbWo4vaBEqQbv9vrhOFy3a5hkloPftbacQN4mKeH0LMcNS7OqyantP740%2FcZQSBdjQ6oTq7nH7hm0nU64vMlVqQHmjW0bsbxC%2FtmCWeCWPU82IvDkGxpYAcivyRxpERd%2BeWbKJcJSGHzsrwd6JGCDRTt9BJmX0iEctxw0x9xNMWGa9jmblakmDAoUiX2nvNbNp1xXvBFm5XA%2Fw6FFJPxn37OTJN34OqtQRaJVKBF3xRUJxOyPQv4jCRJ1CInQhm7Io3ChwSfv9x0lrqnAT4wjfv2YbCAcw8ryLhAY67AE4kTN804U%2FrSusb9LPolgyjfmKqZw%2BFWfe1evQ14tHaUms7swrxoxHg3NL08PZ99kQdZAIXvBXecroEszxbcTr81p6jPCeTT%2B6KWvmQTZCQaGYpvfY0%2FCSCD9gqNwofnwvqfMFDPSrFmZ5xW027OOMuFOiG8K%2BLFp02dGxg73ZxngvoWvdmAeeDn1sudODVkn7KQZzISVpvw5n0FJ3zLiI8%2BYh%2FP5zZzXJv5H0%2B1nvCagqxBVQLFLM%2FQPK0%2BtJJVw%2FAQkZRLuw3rEVOpVwkwigqdO%2BKnw4kSw8ijQ85%2FcFJqYeAHDO6ikqyzNXxg%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20210423T155256Z&X-Amz-SignedHeaders=host&X-Amz-Expires=14400&X-Amz-Credential=ASIAWSMX3SNWZ7S6OYNW%2F20210423%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Signature=3386e0c7f55082113946fa967d897eaea552c37224a1782cc7730c5611f8d9bc [following]\n",
            "--2021-04-23 15:52:56--  https://uc3-s3mrt1001-prd.s3.us-west-2.amazonaws.com/e4507710-8648-4024-99da-d2cd82fc2b94/data?response-content-disposition=attachment%3B%20filename%3Ddoi_10.5061_dryad.p8s0j__v1.zip&response-content-type=application%2Fzip&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEG8aCXVzLXdlc3QtMiJGMEQCIFbd%2BOCWHKBgjxS2z%2B87%2B3zDAdUUp1KUZEX9AOeeYtkdAiBSbYhuCyETTNoMGthEFHSybLmGmK5W2b2IOXZHhMS7Iyq9AwjY%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAAaDDQ1MTgyNjkxNDE1NyIMXm6%2FikkAO1now%2F1LKpED1MoRR7EvItVKSWE6KSOuLz9857PUOrxRkMBVvOdLedTIg1xn%2F8dPmkbGV%2FMHDd4UI%2FWyz6US6FRNueExqSlMgXxpV4oAWeaU4YYcnn6dOYUdEAR9j3nWgCXX4tkWrkZJYqPrj9EX%2FiISmwSubUpQa4Af721iLnree7MxYtG8iUoESvkGUkbiQlwv7wQcbVZkgqZOy3M3tit%2F5sorLgCJflxoxj%2FeJK4RLLWTxLE2ZN02QyJCT%2F5bq0IARErbWo4vaBEqQbv9vrhOFy3a5hkloPftbacQN4mKeH0LMcNS7OqyantP740%2FcZQSBdjQ6oTq7nH7hm0nU64vMlVqQHmjW0bsbxC%2FtmCWeCWPU82IvDkGxpYAcivyRxpERd%2BeWbKJcJSGHzsrwd6JGCDRTt9BJmX0iEctxw0x9xNMWGa9jmblakmDAoUiX2nvNbNp1xXvBFm5XA%2Fw6FFJPxn37OTJN34OqtQRaJVKBF3xRUJxOyPQv4jCRJ1CInQhm7Io3ChwSfv9x0lrqnAT4wjfv2YbCAcw8ryLhAY67AE4kTN804U%2FrSusb9LPolgyjfmKqZw%2BFWfe1evQ14tHaUms7swrxoxHg3NL08PZ99kQdZAIXvBXecroEszxbcTr81p6jPCeTT%2B6KWvmQTZCQaGYpvfY0%2FCSCD9gqNwofnwvqfMFDPSrFmZ5xW027OOMuFOiG8K%2BLFp02dGxg73ZxngvoWvdmAeeDn1sudODVkn7KQZzISVpvw5n0FJ3zLiI8%2BYh%2FP5zZzXJv5H0%2B1nvCagqxBVQLFLM%2FQPK0%2BtJJVw%2FAQkZRLuw3rEVOpVwkwigqdO%2BKnw4kSw8ijQ85%2FcFJqYeAHDO6ikqyzNXxg%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20210423T155256Z&X-Amz-SignedHeaders=host&X-Amz-Expires=14400&X-Amz-Credential=ASIAWSMX3SNWZ7S6OYNW%2F20210423%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Signature=3386e0c7f55082113946fa967d897eaea552c37224a1782cc7730c5611f8d9bc\n",
            "Resolving uc3-s3mrt1001-prd.s3.us-west-2.amazonaws.com (uc3-s3mrt1001-prd.s3.us-west-2.amazonaws.com)... 52.218.243.57\n",
            "Connecting to uc3-s3mrt1001-prd.s3.us-west-2.amazonaws.com (uc3-s3mrt1001-prd.s3.us-west-2.amazonaws.com)|52.218.243.57|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 101904043 (97M) [application/zip]\n",
            "Saving to: ‘article-urls.zip’\n",
            "\n",
            "article-urls.zip    100%[===================>]  97.18M  28.3MB/s    in 3.8s    \n",
            "\n",
            "2021-04-23 15:53:00 (25.9 MB/s) - ‘article-urls.zip’ saved [101904043/101904043]\n",
            "\n",
            "File Name                                             Modified             Size\n",
            "article-urls.tar.gz                            2021-04-22 01:39:24    101872833\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_KRJ_-DwAoq",
        "scrolled": true
      },
      "source": [
        "res = pd.read_csv('article-urls.csv', error_bad_lines=False,encoding = 'unicode_escape',names = ['URL'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkWxWV1RwAor"
      },
      "source": [
        "URLs are obviously a (very) bad source of information. As can be seen below, much of the information (if there is any..) is squeezed between random symbols and strings. In addition we can already see that depending on the website this information is not present in the same way, which will cause more trouble for the pre-processing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6aR0LLwwAor",
        "outputId": "02313893-8838-4a1e-e336-d1573cf65c73"
      },
      "source": [
        "res['URL'].values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['1\\thttp://www.nzherald.co.nz/world/news/article.cfm?c_id=2&objectid=11405178&ref=rss',\n",
              "       '2\\thttp://dialog.ua.edu/2014/03/the-myth-of-martyrdom/',\n",
              "       '3\\thttp://www.thehindu.com/news/cities/Delhi/over-200-fireworks-stalls-gutted-in-faridabad/article6525578.ece?utm_source=RSS_Feed&utm_medium=RSS&utm_campaign=RSS_Syndication',\n",
              "       ...,\n",
              "       '2353604\\thttp://sputniknews.com/europe/20150404/1020476864.html',\n",
              "       '2353605\\thttp://www.marketwatch.com/news/story.asp?guid=%7B18A5C462-B061-11E4-9E60-FD2AF902D49F%7D&siteid=rss&rss=1',\n",
              "       '2353606\\thttp://theage.com.au/business/small-business/trends/whos-making-money-from-airbnb-20150119-12riqh.html'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbw0Bi0lwAos"
      },
      "source": [
        "First step of cleaning: removing http:// (and www. if present). We then split these URL's according to / and - to access the individual words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXH0_7rYwAot"
      },
      "source": [
        "# remove_pattern takes an url - a long string, and returns new_url_array the part to the right of pattern\n",
        "# (pattern will be http or www)\n",
        "\n",
        "def remove_pattern(url_array,pattern):\n",
        "    new_url_array = np.copy(url_array)\n",
        "    for i in range(len(url_array)):\n",
        "        new_url = re.split(pattern,url_array[i])[-1]\n",
        "        new_url_array[i] = new_url\n",
        "    return new_url_array     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jw_5Vyz6wAot"
      },
      "source": [
        "# split_pattern takes an url (string) and returns an array of string, which were separated \n",
        "# between each other by pattern ( - or /)\n",
        "\n",
        "def split_pattern(url_array,pattern):\n",
        "    new_url_array = np.empty(len(url_array),np.ndarray)\n",
        "    for i in range(len(url_array)):\n",
        "        new_url = re.split(pattern,url_array[i])\n",
        "        new_url_array[i] = np.array(new_url)\n",
        "    return new_url_array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvJWh75fwAot"
      },
      "source": [
        "Getting rid of all url artifacts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6szt-90wAou"
      },
      "source": [
        "pattern1 = 'http://'\n",
        "pattern2 = 'www.'\n",
        "patt = '[/-]'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r17Yjqi9wAou"
      },
      "source": [
        "Below we run these cleaning functions and show what they do to a random url"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWRiZRD4wAou",
        "outputId": "ce5c5413-7b3c-4ac5-c6c4-e6306765efb6"
      },
      "source": [
        "l = 320\n",
        "url_array = remove_pattern(res['URL'].values,pattern1)\n",
        "print(url_array[l])\n",
        "url_array = remove_pattern(url_array,pattern2)\n",
        "print(url_array[l])\n",
        "url_array = split_pattern(url_array,patt)\n",
        "print(url_array[l])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "www.irishtimes.com/news/crime-and-law/courts/high-court/brian-o-donnell-legal-action-has-no-valid-basis-court-hears-1.2147241\n",
            "irishtimes.com/news/crime-and-law/courts/high-court/brian-o-donnell-legal-action-has-no-valid-basis-court-hears-1.2147241\n",
            "['irishtimes.com' 'news' 'crime' 'and' 'law' 'courts' 'high' 'court'\n",
            " 'brian' 'o' 'donnell' 'legal' 'action' 'has' 'no' 'valid' 'basis' 'court'\n",
            " 'hears' '1.2147241']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYBIpmuMwAou"
      },
      "source": [
        "Now we must get rid of fake words (those with common non alpha numerical character within)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RfHKIqEwAov"
      },
      "source": [
        "for i in range(len(url_array)):\n",
        "    url_array[i] = np.array(list(filter(lambda x: not(re.search('[_=~]',x)),url_array[i])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fucHkHEowAov",
        "outputId": "16962ad1-fdf5-496d-8515-464310f81b14"
      },
      "source": [
        "for i in range(0,1000,200):\n",
        "    print(url_array[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['nzherald.co.nz' 'world' 'news']\n",
            "['feedproxy.google.com' 'HeraldSunTopStories' 'WdeK4TUVcWc' 'story'\n",
            " 'fni0fit3' '1227220701587']\n",
            "['thehindu.com' 'features' 'metroplus' 'it' 'happens' 'only' 'in'\n",
            " 'bengaluru']\n",
            "['newsok.com' 'three' 'die' 'after' 'oklahoma' 'crashes' 'article']\n",
            "['jamaica' 'star.com' 'thestar' '20150323' 'news' 'news2.html']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmxSEb9-wAov"
      },
      "source": [
        "### Extracting website name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0WySVwJwAov"
      },
      "source": [
        "The website name is rather easily extracted and will be used as a feature later on. This script is not perfect as some websites changer their domain name for some articles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjSVa9OFwAow"
      },
      "source": [
        "s = []\n",
        "origin = []\n",
        "for i in range(len(url_array)):\n",
        "    if len(url_array[i]) == 0:\n",
        "        s.append(i)\n",
        "        origin.append('Bad website')\n",
        "        continue\n",
        "        \n",
        "    origin.append(url_array[i][0])\n",
        "    url_array[i] = url_array[i][1:]\n",
        "    \n",
        "origin = np.array(origin)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdYI7lB4wAow"
      },
      "source": [
        "We make sure each string is an actual word by suppressing strings with an uppercase letter within or with non letters. We also suppress a few un-informative words (handpicked) such as news which is present everywhere in the news."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foCBdwgNwAow"
      },
      "source": [
        "forbidden_words = ['news','article','story','world','articles']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDocGqV5wAox"
      },
      "source": [
        "for i in range(len(url_array)):\n",
        "    new_array = []\n",
        "    for j in range(len(url_array[i])):\n",
        "        if url_array[i][j].isalpha() and not(re.match('.[A-Z]',url_array[i][j])):\n",
        "            if url_array[i][j] not in forbidden_words:\n",
        "                new_array.append(url_array[i][j])\n",
        "    url_array[i] = np.array(new_array)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L19bKxiwwAox"
      },
      "source": [
        "for i in range(len(url_array)):\n",
        "    if len(url_array[i]) == 0:\n",
        "        origin[i] = 'Bad website'\n",
        "        \n",
        "## bad website = url without any information (=words) left, or only one or two"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75CulVifwAox"
      },
      "source": [
        "## this merges the urls back together to form sentences\n",
        "for i in range(len(url_array)):\n",
        "    url_array[i] = ' '.join(url_array[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-voDv3HwAox",
        "outputId": "18d0765f-12f8-4158-ebe1-ddb12eefd1e3"
      },
      "source": [
        "for i in range(0,1000,100):\n",
        "    print(url_array[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "the cycle poll hillary clinton most admired woman\n",
            "HeraldSunTopStories\n",
            "good weather pebble good golf john daly\n",
            "features metroplus it happens only in bengaluru\n",
            "Councillors seeking rethink Sir Peter Soulsby s detail\n",
            "three die after oklahoma crashes\n",
            "newscomaubreakingndm\n",
            "thestar\n",
            "michel chossudovsky towards a war iii scenario now available through kindle\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1s4RlUfxwAoy"
      },
      "source": [
        "As can be seen above, we did a decent job of cleaning up urls to obtain only meaningfull words. But we will now look at other methods to further clean (tokenizer, etc..)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWNpDbRqwAoy"
      },
      "source": [
        "### Second cleaning with tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "PRxKhgUawAoy",
        "outputId": "dd1fde83-6eaa-45af-8444-fbfce881c5ef"
      },
      "source": [
        "nltk.download('words')\n",
        "words = set(nltk.corpus.words.words())\n",
        "\n",
        "sent = url_array[0]\n",
        "\" \".join(w for w in nltk.wordpunct_tokenize(sent) \\\n",
        "         if w.lower() in words or not w.isalpha())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "''"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqD4fBrmwAoy"
      },
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer, TweetTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAysQNQTwAoz",
        "outputId": "603aed17-8538-466a-f715-80138122c7a2"
      },
      "source": [
        "from gensim.models.phrases import Phrases, Phraser\n",
        "from nltk.tokenize import TreebankWordTokenizer, TweetTokenizer\n",
        "nltk.download('punkt')\n",
        "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "\n",
        "\n",
        "def tokenize_url_hashtags(corpus, tweets=False):\n",
        "    if tweets:\n",
        "        tokenizer = TweetTokenizer()\n",
        "    else:\n",
        "        tokenizer = TreebankWordTokenizer()  \n",
        "        tokenized_sentences = []\n",
        "    for sample in tqdm(corpus):\n",
        "    # separating sentences\n",
        "        for sentence in sent_detector.tokenize(sample):\n",
        "            tokens = tokenizer.tokenize(sentence)\n",
        "            tokens = list(map(lambda x: x.lower(), tokens))\n",
        "            tokenized_sentences.append(tokens)\n",
        "    return tokenized_sentences"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmvFKwX7wAoz",
        "outputId": "2dcf8fe5-6593-48b2-ed92-a0ef5b9f3d0e"
      },
      "source": [
        "tok = tokenize_url_hashtags(url_array, tweets=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2353606/2353606 [03:20<00:00, 11751.03it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjiI7ToYwAoz"
      },
      "source": [
        "We drop urls with no information (= Bad website). The nltk tokenizer already did that to tok, which is why we get rid of these websites only for origine."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "id": "ixC1oBnrwAoz",
        "outputId": "91fc43ee-7786-481f-a8a1-e21d2a0e8505"
      },
      "source": [
        "cleaned_res = pd.DataFrame(data = np.array([origin[origin != 'Bad website'],tok]).T,\n",
        "                                     columns = ['Website','Cleaned URL'])\n",
        "cleaned_res"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Website</th>\n",
              "      <th>Cleaned URL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>dialog.ua.edu</td>\n",
              "      <td>[the, myth, of, martyrdom]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>thehindu.com</td>\n",
              "      <td>[cities, delhi, over, fireworks, stalls, gutte...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>arkansasonline.com</td>\n",
              "      <td>[feb, chilly, air, grips, state, snow, possibl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>theguardian.com</td>\n",
              "      <td>[politics, dec, nick, clegg, pmqs, increasingl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>feedproxy.google.com</td>\n",
              "      <td>[dailymail, home, k, shoppers, final, xmas]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2189838</th>\n",
              "      <td>theage.com.au</td>\n",
              "      <td>[afl, afl, ryan, griffen, already, paying, his...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2189839</th>\n",
              "      <td>dissidentvoice.org</td>\n",
              "      <td>[what, did, people, do, before, the, war, daddy]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2189840</th>\n",
              "      <td>wkzo.com</td>\n",
              "      <td>[oct, us, national, security, prosecutors, shi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2189841</th>\n",
              "      <td>sputniknews.com</td>\n",
              "      <td>[europe]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2189842</th>\n",
              "      <td>theage.com.au</td>\n",
              "      <td>[business, small, business, trends, whos, maki...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2189843 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                      Website                                        Cleaned URL\n",
              "0               dialog.ua.edu                         [the, myth, of, martyrdom]\n",
              "1                thehindu.com  [cities, delhi, over, fireworks, stalls, gutte...\n",
              "2          arkansasonline.com  [feb, chilly, air, grips, state, snow, possibl...\n",
              "3             theguardian.com  [politics, dec, nick, clegg, pmqs, increasingl...\n",
              "4        feedproxy.google.com        [dailymail, home, k, shoppers, final, xmas]\n",
              "...                       ...                                                ...\n",
              "2189838         theage.com.au  [afl, afl, ryan, griffen, already, paying, his...\n",
              "2189839    dissidentvoice.org   [what, did, people, do, before, the, war, daddy]\n",
              "2189840              wkzo.com  [oct, us, national, security, prosecutors, shi...\n",
              "2189841       sputniknews.com                                           [europe]\n",
              "2189842         theage.com.au  [business, small, business, trends, whos, maki...\n",
              "\n",
              "[2189843 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wreHKzxbwAo0"
      },
      "source": [
        "URLs with too few words are often low quality"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEsPGSmhwAo0"
      },
      "source": [
        "ind = np.array([True for i in range(len(cleaned_res))])\n",
        "\n",
        "for i in range(len(cleaned_res['Cleaned URL'])):\n",
        "    if len(cleaned_res['Cleaned URL'][i]) < 3:\n",
        "        ind[i] = False\n",
        "cleaned_res = cleaned_res[ind]\n",
        "cleaned_res.index = range(0,len(cleaned_res))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRHbISyOwAo0"
      },
      "source": [
        "Getting rid of low-quality websites that have few links (less than 100). Indeed we can do any statistics on those websites."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "PhVy_UzuwAo0",
        "outputId": "d7178cc5-2ab6-4a4a-dae7-fb88a05df316"
      },
      "source": [
        "web = cleaned_res.groupby(['Website']).count()\n",
        "web.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Cleaned URL</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Website</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>.med.umich.edu</th>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1000155\\t</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100256\\t</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1002759\\t</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                Cleaned URL\n",
              "Website                    \n",
              "                          2\n",
              ".med.umich.edu           10\n",
              "1000155\\t                 1\n",
              "100256\\t                  1\n",
              "1002759\\t                 1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "Ps9EfxFEwAo1",
        "outputId": "98634002-9811-4ef0-9bd3-f605472f5d2d"
      },
      "source": [
        "plt.hist(np.log10(web.values))\n",
        "plt.title('Histogram showing log_10 number of articles for a website')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4nYVd_VwAo1"
      },
      "source": [
        "Many of the 'websites' we have are probably mistakes (due to a domain name that changes). We thus toss them out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOMdXPmpwAo1",
        "outputId": "7f02169e-779e-4105-d348-71f6da8962b8"
      },
      "source": [
        "ind_li = []\n",
        "for i in tqdm(range(len(web))):\n",
        "    if web.iloc[i,0] < 100:\n",
        "        ind_li.append(\n",
        "            np.array(list((cleaned_res[cleaned_res['Website'] == web.index[i]]).index))\n",
        "        )\n",
        "        \n",
        "ind_li = np.concatenate(ind_li).astype(int)\n",
        "cleaned_res = cleaned_res.drop(ind_li)\n",
        "\n",
        "cleaned_res.index = range(0,len(cleaned_res))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 3882/3882 [06:18<00:00, 10.26it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "MCDiFoVLwAo2",
        "outputId": "0c37150b-2c54-477a-de74-46b94284352d"
      },
      "source": [
        "cleaned_res"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Website</th>\n",
              "      <th>Cleaned URL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>dialog.ua.edu</td>\n",
              "      <td>[the, myth, of, martyrdom]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>thehindu.com</td>\n",
              "      <td>[cities, delhi, over, fireworks, stalls, gutte...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>arkansasonline.com</td>\n",
              "      <td>[feb, chilly, air, grips, state, snow, possibl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>theguardian.com</td>\n",
              "      <td>[politics, dec, nick, clegg, pmqs, increasingl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>feedproxy.google.com</td>\n",
              "      <td>[dailymail, home, k, shoppers, final, xmas]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1806061</th>\n",
              "      <td>zeenews.india.com</td>\n",
              "      <td>[ferguson, policies, targeted, blacks, created...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1806062</th>\n",
              "      <td>theage.com.au</td>\n",
              "      <td>[afl, afl, ryan, griffen, already, paying, his...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1806063</th>\n",
              "      <td>dissidentvoice.org</td>\n",
              "      <td>[what, did, people, do, before, the, war, daddy]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1806064</th>\n",
              "      <td>wkzo.com</td>\n",
              "      <td>[oct, us, national, security, prosecutors, shi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1806065</th>\n",
              "      <td>theage.com.au</td>\n",
              "      <td>[business, small, business, trends, whos, maki...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1806066 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                      Website                                        Cleaned URL\n",
              "0               dialog.ua.edu                         [the, myth, of, martyrdom]\n",
              "1                thehindu.com  [cities, delhi, over, fireworks, stalls, gutte...\n",
              "2          arkansasonline.com  [feb, chilly, air, grips, state, snow, possibl...\n",
              "3             theguardian.com  [politics, dec, nick, clegg, pmqs, increasingl...\n",
              "4        feedproxy.google.com        [dailymail, home, k, shoppers, final, xmas]\n",
              "...                       ...                                                ...\n",
              "1806061     zeenews.india.com  [ferguson, policies, targeted, blacks, created...\n",
              "1806062         theage.com.au  [afl, afl, ryan, griffen, already, paying, his...\n",
              "1806063    dissidentvoice.org   [what, did, people, do, before, the, war, daddy]\n",
              "1806064              wkzo.com  [oct, us, national, security, prosecutors, shi...\n",
              "1806065         theage.com.au  [business, small, business, trends, whos, maki...\n",
              "\n",
              "[1806066 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iVOpx9AwAo2"
      },
      "source": [
        "The cleaning is over, we will now process the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfWfx22XwAo2"
      },
      "source": [
        "cleaned_res.to_pickle('cleaned_res.pkl') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBVFiYwQwAo2"
      },
      "source": [
        "# 2. Word level embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMqRRx-MwAo2"
      },
      "source": [
        "Start from here without running the previous code if cleaned_res.pkl already exists to save time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DOzqxjCwAo3"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcKG3-RLwAo3"
      },
      "source": [
        "cleaned_res = pd.read_pickle('cleaned_res.pkl')\n",
        "tok = cleaned_res['Cleaned URL']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LaaothSwAo3"
      },
      "source": [
        "Now that our url's are cleaned we will train a Word2vec to embed our data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Zf1AqHXwAo3",
        "outputId": "f1d5277e-cc5e-4a53-cf42-c9d6d323f088"
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "from multiprocessing import cpu_count\n",
        "\n",
        "cpu = cpu_count()\n",
        "\n",
        "print(\"Training the URL W2V ...\")\n",
        "url = Word2Vec(tok, size=100, window=5, min_count=3, workers=cpu)\n",
        "url.train(tok, total_examples=len(tok), epochs=10)\n",
        "\n",
        "wv = url.wv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training the URL W2V ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SwM-bzQ0wAo3",
        "outputId": "125d3d49-a3db-4a91-9462-1fd423376b29"
      },
      "source": [
        "print('There are '+str(len(wv.vocab.keys())) + ' unique words in this dataset!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 85740 unique words in this dataset!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsozEJmbwAo4"
      },
      "source": [
        "## Qualitative analysis of this embedding\n",
        "Let's look at words most similar to 'selfie' and 'war' to get a feel of how accurate the embedding is.\n",
        "At first glance it looks accurate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bJVqz-XzwAo4",
        "scrolled": false,
        "outputId": "0a1cd8de-2e3b-4d65-e48e-d0e9c9086703"
      },
      "source": [
        "wv.most_similar('selfie', topn = 10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('selfies', 0.675609290599823),\n",
              " ('photo', 0.5234270691871643),\n",
              " ('picture', 0.5227267146110535),\n",
              " ('tattoos', 0.521005392074585),\n",
              " ('photoshoot', 0.5145403146743774),\n",
              " ('blonde', 0.5016199946403503),\n",
              " ('photograph', 0.4911169409751892),\n",
              " ('onesie', 0.4898644685745239),\n",
              " ('latex', 0.4852360785007477),\n",
              " ('photoshop', 0.46837329864501953)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZEE_XKcawAo4",
        "scrolled": true,
        "outputId": "2b7201a6-ef07-4e16-c3fe-81733cdebfb6"
      },
      "source": [
        "wv.most_similar('war', topn = 10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('conflict', 0.5586923360824585),\n",
              " ('disobedience', 0.5394599437713623),\n",
              " ('atrocities', 0.5114937424659729),\n",
              " ('ww', 0.49922433495521545),\n",
              " ('aggression', 0.4788926839828491),\n",
              " ('humanity', 0.47490280866622925),\n",
              " ('impunity', 0.4623667597770691),\n",
              " ('liberties', 0.44553422927856445),\n",
              " ('battle', 0.4449853301048279),\n",
              " ('occupation', 0.4429374933242798)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0PDWBtdwAo4"
      },
      "source": [
        "## How do we vectorize our url's?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kacMAsPYwAo4"
      },
      "source": [
        "As in lab 3 we first summarize an url by the mean of its tokens ($\\frac{1}{n} \\Sigma x_i$) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "apLTGW2TwAo5"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn import preprocessing\n",
        "import numpy as np\n",
        "\n",
        "def tokens2vectors(tokenCorpus, trained_word2vec, dim=100):\n",
        "    new_sample = list()\n",
        "    for sample in tqdm(tokenCorpus):\n",
        "        tweetVecs = list()\n",
        "        for token in sample.split(' '):\n",
        "            try:\n",
        "                tweetVecs.append(trained_word2vec.get_vector(token)  )\n",
        "            except:\n",
        "                tweetVecs.append(np.zeros(dim) ) \n",
        "\n",
        "        new_sample.append(np.mean(tweetVecs, axis=0))\n",
        "    return np.array(new_sample)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "uPFGeRtBwAo5"
      },
      "source": [
        "## the tokens2vectors functions takes whole sequences into account\n",
        "\n",
        "tok_val = tok.values\n",
        "\n",
        "for i in range(len(tok)):\n",
        "    tok_val[i] = ' '.join(tok_val[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "L1ex-3kWwAo5",
        "outputId": "1fcef193-0eb1-4b1e-b924-96f395c504a4"
      },
      "source": [
        "X = tokens2vectors(tok_val,wv)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1806066/1806066 [01:21<00:00, 22027.23it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKfRwqFxwAo5"
      },
      "source": [
        "### We then use the PCA decomposition to get quantitative measures of embedding quality"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-25vQ6uwAo5"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "pca.fit(X)\n",
        "y = pca.transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4_eNXlJHwAo6"
      },
      "source": [
        "y = pca.transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cQomRUywAo6"
      },
      "source": [
        "cleaned_res['pca1'] = y.T[0]\n",
        "cleaned_res['pca2'] = y.T[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q332bVYowAo6"
      },
      "source": [
        "Are these pca components usefull? Let's look at two random newspapers and see if their urls (through their pca components) are separated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dF3gqYQwAo6"
      },
      "source": [
        "zeenew_pca = cleaned_res[cleaned_res['Website'] == 'zeenews.india.com' ][['pca1','pca2']].values\n",
        "thestar_pca = cleaned_res[cleaned_res['Website'] == 'thestar.com.my' ][['pca1','pca2']].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iue9fo-wwAo6"
      },
      "source": [
        "plt.figure(figsize=(9,9))\n",
        "plt.scatter(y.T[0],y.T[1])\n",
        "plt.scatter(zeenew_pca.T[0],zeenew_pca.T[1])\n",
        "plt.scatter(thestar_pca.T[0],thestar_pca.T[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIECuVgiwAo7"
      },
      "source": [
        "This is not that surprizing: most newspapers are generalists.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtIbhHTNwAo7"
      },
      "source": [
        "# 3. Clustering our dataset to detect topics in an unsupervized way"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8uFv6gzwAo7"
      },
      "source": [
        "Because we work with unlabelled data we have to resort to unsupervized learning. Our hypothesis is slightly changed for now. Let's see if we can detect interesting clusters which might relate to different topics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYe03d0vwAo7"
      },
      "source": [
        "We use the elbow method to find an adequate number of clusters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OotHJaz5wAo7"
      },
      "source": [
        "K = range(6,22)\n",
        "distortions = []\n",
        "\n",
        "#\"\"\"\" Very long computation!!\n",
        "for k in tqdm(K):\n",
        "    kmeanModel = KMeans(n_clusters=k,max_iter=8, verbose=1,n_init=1)\n",
        "    kmeanModel.fit(X)\n",
        "    distortions.append(kmeanModel.inertia_)\n",
        "#     \"\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rM_mTIGwwAo8"
      },
      "source": [
        "plt.plot(K,distortions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNTqzVx0wAo8"
      },
      "source": [
        "### What kind of clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12VybDnywAo8"
      },
      "source": [
        "This typical clustering method is perhaps not so adequate in our situation. By using a larger and larger amount of clusters we might well simply uncover subcategories of existing topics. For instance Rugby and Soccer are both subcategories of Sports."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EXnT4B7wAo8"
      },
      "source": [
        "We will try two different numbers of clusters: 8 and 40. Hopefully the first"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMpwmZ9LwAo8"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "# this example uses only 3 initializations to be fast (but less effective!)\n",
        "kmeans_8 = KMeans(n_clusters=8, random_state=42, n_init=1, max_iter=100, verbose=1).fit(X)\n",
        "\n",
        "kmeans_40 = KMeans(n_clusters=40, random_state=42, n_init=1, max_iter=100, verbose=1).fit(X)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydS4PrhZwAo9"
      },
      "source": [
        "clusters_8 = kmeans_8.predict(X)\n",
        "clusters_40 = kmeans_40.predict(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FIX3EwHwAo9"
      },
      "source": [
        "What kind of words are present in each centroid?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gk1tpf17wAo9"
      },
      "source": [
        "vocab = np.array(list(wv.vocab.keys())) # contains all the vocabulary present in the URLs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTpCtOdhwAo9"
      },
      "source": [
        "vocab_embed = np.zeros((len(vocab),100)) #vocab_embed represents the embedding for each word in the vocab\n",
        "for i in range(len(vocab)):\n",
        "    vocab_embed[i] = wv.get_vector(vocab[i])  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h308Mya8wAo9"
      },
      "source": [
        "# taking an instantated kmeans and the clusters, this function examines their centroids    \n",
        "def look_at_centroids(kmean, clusters): \n",
        "    for i in range(len(kmean.cluster_centers_)):\n",
        "        cent = kmean.cluster_centers_[i]\n",
        "        res = []\n",
        "        for j in range(len(vocab_embed)):\n",
        "            res.append(np.linalg.norm(cent - vocab_embed[j] ))\n",
        "        indices = [x for _, x in sorted(zip(res,range(len(res))))]\n",
        "        print('Top 10 words closest to centroid center number: ' + str(i))\n",
        "        print(vocab[indices[:10]])\n",
        "        \n",
        "        url_ind = np.argmin(clusters!= i)\n",
        "        print()\n",
        "        print('Sample URL contained in this cluster, coming from \"'+ cleaned_res['Website'][url_ind] + '\": ')\n",
        "        print('\" '+cleaned_res['Cleaned URL'][url_ind]+' \" ')\n",
        "        print()\n",
        "        print(35*'-')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SYi_AuqwAo-"
      },
      "source": [
        "Comparing the 8 and the 40 clusters the improvement is obvious."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grq3BxupwAo-",
        "scrolled": true
      },
      "source": [
        "look_at_centroids(kmeans_8,clusters_8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSEcTZo_wAo-",
        "scrolled": true
      },
      "source": [
        "look_at_centroids(kmeans_40,clusters_40)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_tQqT2BwAo-"
      },
      "source": [
        "### How are these clusters balanced?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y43e4a1mwAo-",
        "scrolled": true
      },
      "source": [
        "plt.hist(clusters_8)\n",
        "plt.title('Histogram of urls per cluster  (8 Clusters) ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKO7pSXhwAo_"
      },
      "source": [
        "plt.hist(clusters_40)\n",
        "plt.title('Histogram of urls per cluster  (40 Clusters) ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoVNJQphwAo_"
      },
      "source": [
        "cleaned_res['Cluster_8'] = clusters_8\n",
        "cleaned_res['Cluster_40'] = clusters_40"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OM5ZQKdzwAo_"
      },
      "source": [
        "cleaned_res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yux8Dx1JwAo_"
      },
      "source": [
        "new_web = cleaned_res.groupby(['Website']).count()['Cleaned URL']\n",
        "new_web"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVnldyOe-hJd"
      },
      "source": [
        "cleaned_res.to_pickle('clustered_cleaned_res.pkl') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFjkca4xwApA"
      },
      "source": [
        "## Looking at the proportion of clusters for each website"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ul10TNJHwApA"
      },
      "source": [
        "nytimes = cleaned_res[cleaned_res['Website'] == 'nytimes.com']\n",
        "forbes = cleaned_res[cleaned_res['Website'] == 'forbes.com']\n",
        "hindu = cleaned_res[cleaned_res['Website'] == 'thehindu.com']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7W1NfzGwApA"
      },
      "source": [
        "plt.hist(nytimes['Cluster_40'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hne8D5Z2wApA"
      },
      "source": [
        "nytimes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLvut8JYwApA"
      },
      "source": [
        "c4 = cleaned_res[cleaned_res['Cluster_40'] == 4]\n",
        "c2 = cleaned_res[cleaned_res['Cluster_40'] == 2]\n",
        "c0 = cleaned_res[cleaned_res['Cluster_40'] == 0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xwrsx64JwApB"
      },
      "source": [
        "c4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzbtqMx53N1T"
      },
      "source": [
        "# 4. Sentiment Analysis on the articles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2kMrnBH3g5i"
      },
      "source": [
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "nltk.download('vader_lexicon')\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "cleaned_res = pd.read_pickle('clustered_cleaned_res.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuudCHimHLB9"
      },
      "source": [
        "We will now choose a cluster and study the way the sites talk about its related topics. For this, since we do not have a training base for sentiment analysis, we will directly use the pre-trained base offered by the nltk package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caviQEM3HqWM"
      },
      "source": [
        "# Selects a cluster obtained by k-means and predicts the trend of the articles\n",
        "\n",
        "def cluster_sentiment(kmean, c) :\n",
        "\n",
        "  cluster = cleaned_res[cleaned_res['Cluster_40'] == c][['Website','Cleaned URL']]\n",
        "\n",
        "  print(\"####################################################\")\n",
        "  print(\"STUDIED CLUSTER : \", c)\n",
        "  print(\"####################################################\")\n",
        "\n",
        "  cent = kmean.cluster_centers_[c]\n",
        "  res = []\n",
        "  for j in range(len(vocab_embed)):\n",
        "    res.append(np.linalg.norm(cent - vocab_embed[j] ))\n",
        "  indices = [x for _, x in sorted(zip(res,range(len(res))))]\n",
        "  print('Top 10 words closest to centroid center number: ' + str(c))\n",
        "  print(vocab[indices[:10]])\n",
        "  print('----------------------------------------------------')\n",
        "\n",
        "  Sentiment = cluster['Cleaned URL'].apply(lambda x : sia.polarity_scores(\"\".join(x)))\n",
        "  cluster.insert(len(cluster.columns),'Sentiment',Sentiment)\n",
        "\n",
        "  for elem in ['neg','neu','pos','compound'] :\n",
        "    cluster.insert(len(cluster.columns),elem,cluster['Sentiment'].apply(lambda x : x[elem]))\n",
        "\n",
        "  cluster.insert(len(cluster.columns),'Trend',cluster['compound'].apply(lambda x : \"Positive\" if x > 0 else (\"Negative\" if x < 0 else \"Neutral\")))\n",
        "\n",
        "  print(cluster.value_counts('Trend'))\n",
        "  print('----------------------------------------------------')\n",
        "\n",
        "  cluster = cluster[cluster['compound'] != 0]\n",
        "  cluster.insert(len(cluster.columns),'Trend_int',cluster['Trend'].apply(lambda x : 1 if x == 'Positive' else 0))\n",
        "\n",
        "  return cluster"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdTPCiWboP6M"
      },
      "source": [
        "cluster_sentiment(kmeans_40,5).head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVd-uqkwfifE"
      },
      "source": [
        "## First method : Compound discrimination"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMisZjXR0fAY"
      },
      "source": [
        "The first step is to display the frequency distribution of positive articles per newspaper in a given cluster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Dc_YnEepmeh"
      },
      "source": [
        "def plot_hist_compound(cluster, c = -1, key_words = []) :\n",
        "\n",
        "  affinity = cluster[['Website','Trend_int']].groupby('Website').mean()\n",
        "\n",
        "  count, bins_count = np.histogram(affinity)\n",
        "\n",
        "  pdf = count / sum(count)\n",
        "  cdf = np.cumsum(pdf)\n",
        "  \n",
        "  if c >= 0 :\n",
        "    plt.figure(\"Cluster \" + str(c))\n",
        "    plt.plot(bins_count[1:], pdf, color=\"red\", label=\"PDF\")\n",
        "    plt.plot(bins_count[1:], cdf, color=\"blue\", label=\"CDF\")\n",
        "    plt.title(\"PDF and CDF of cluster number \" + str(c))\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "  else :\n",
        "    plt.figure(\"Cluster with key words : \" + str(key_words))\n",
        "    plt.plot(bins_count[1:], pdf, color=\"red\", label=\"PDF\")\n",
        "    plt.plot(bins_count[1:], cdf, color=\"blue\", label=\"CDF\")\n",
        "    plt.title(\"PDF and CDF of the cluster\")\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qhRNzhu0XF2"
      },
      "source": [
        "By selecting the typical graph cases that can be obtained, we obtain this kind of distribution:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52KCdb_2kxgl"
      },
      "source": [
        "analyzed_clusters = [7,11,27,34]\n",
        "\n",
        "for c in analyzed_clusters :\n",
        "\n",
        "  cluster = cluster_sentiment(kmeans_40, c)\n",
        "  if len(cluster) > 0 :\n",
        "    plot_hist_compound(cluster,c)\n",
        "  else :\n",
        "    print(\"No non neutral article in the cluster no \" + str(c))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoULvtEArgxo"
      },
      "source": [
        "The graph of cluster 34 shows, for example, that articles close to the keywords 'men', 'attacks', 'policemen' etc, are quite rarely neutral and take a very positive or negative view.\n",
        "\n",
        "However, clusters are not always very meaningful and may group together articles dealing with very different subjects. Thus, instead of doing this work on the clusters obtained previously (and which are not always significant), we can try to choose the articles which contain a given word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikvxLu1X1Df-"
      },
      "source": [
        "def common_word(l1,key_words) :\n",
        "\n",
        "  for word in key_words :\n",
        "    if word in l1 :\n",
        "      return True\n",
        " \n",
        "  return False\n",
        "\n",
        "\n",
        "def thematic_sentiment(cleaned_res, key_words) :\n",
        "\n",
        "  print(\"### Construction of the thematic cluster ###\")\n",
        "  print('Key words used : ', key_words)\n",
        "\n",
        "  cleaned_res.insert(len(cleaned_res.columns),\"Thematic\",cleaned_res['Cleaned URL'].apply(lambda x : common_word(x,key_words)))\n",
        "  print(cleaned_res.value_counts(['Thematic']))\n",
        "  print(\"-----------------------------------------\")\n",
        "  them_cluster = cleaned_res[cleaned_res['Thematic'] == True]\n",
        "  del cleaned_res['Thematic']\n",
        "\n",
        "  Sentiment = them_cluster['Cleaned URL'].apply(lambda x : sia.polarity_scores(\"\".join(x)))\n",
        "  them_cluster.insert(len(them_cluster.columns),'Sentiment',Sentiment)\n",
        "\n",
        "  for elem in ['neg','neu','pos','compound'] :\n",
        "    them_cluster.insert(len(them_cluster.columns),elem,them_cluster['Sentiment'].apply(lambda x : x[elem]))\n",
        "\n",
        "  them_cluster.insert(len(them_cluster.columns),'Trend',them_cluster['compound'].apply(lambda x : \"Positive\" if x > 0 else (\"Negative\" if x < 0 else \"Neutral\")))\n",
        "\n",
        "  print(them_cluster.value_counts('Trend'))\n",
        "  print(\"-----------------------------------------\")\n",
        "\n",
        "  them_cluster = them_cluster[them_cluster['compound'] != 0]\n",
        "  them_cluster.insert(len(them_cluster.columns),'Trend_int',them_cluster['Trend'].apply(lambda x : 1 if x == 'Positive' else 0))\n",
        "\n",
        "  print(\"Size of the thematic cluster : \" + str(len(them_cluster)))\n",
        "\n",
        "  return them_cluster"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p75TMrHZ2g_L"
      },
      "source": [
        "thematics = [[\"bank\",\"finance\"],\n",
        "             [\"social\",\"health\"],\n",
        "             [\"unemployment\",\"employment\",\"work\"],\n",
        "             [\"immigration\",\"refugee\"],\n",
        "             [\"environment\",\"climate\",\"pollution\"]]\n",
        "\n",
        "for key_words in thematics :\n",
        "\n",
        "  them_cluster = thematic_sentiment(cleaned_res, key_words)\n",
        "  plot_hist_compound(them_cluster, c = -1, key_words = key_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVIvL2uZLYmj"
      },
      "source": [
        "For some topics, such as banking and finance, we can see that the sites are broadly divided into three types, visible in the three peaks of the red curve. On the left, sites that publish exclusively negative articles; on the right, sites that use mostly positive expressions. Finally, in the middle, the sites with a neutral tendency."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdlPSHY3Lw7-"
      },
      "source": [
        "## Second method : Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GP131AClL4PY"
      },
      "source": [
        "We can also use the three values obtained for the positivity, neutrality and negativity score and try to cluster the articles on a given theme."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcFTd0EeMVX-"
      },
      "source": [
        "### Prend en entrée un cluster thématique et clusterise en fonction des trois valeurs 'pos', 'neg' et 'neu'\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "def clust(them_cluster) :\n",
        "\n",
        "  print(\"### Construction of the three clusters ###\")\n",
        "  X = []\n",
        "  for i in them_cluster.index :\n",
        "    line = them_cluster.loc[i]\n",
        "    X.append([line['neg'],line['neu'],line['pos']])\n",
        "\n",
        "  kmeans = KMeans(n_clusters=3, random_state=42).fit(X)\n",
        "  clusters_3 = kmeans.predict(X)\n",
        "\n",
        "  them_cluster['Cluster_3'] = clusters_3\n",
        "\n",
        "  return them_cluster"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dMtVk77SWnz"
      },
      "source": [
        "# Affiche les clusters sur les articles\n",
        "\n",
        "def plot_them_cluster(them_cluster) :\n",
        "\n",
        "  plt.figure(figsize=(9,9))\n",
        "  colors = ['red','blue','green']\n",
        "\n",
        "  for k in range(3) :\n",
        "\n",
        "    clust = them_cluster[them_cluster['Cluster_3'] == k]\n",
        "    plt.scatter(clust['pos'], clust['neg'], c = colors[k])\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "# Affiche les articles d'un même site sur l'ensemble des articles\n",
        "\n",
        "def plot_websites(them_cluster, websites) :\n",
        "\n",
        "  plt.figure(figsize=(9,9))\n",
        "\n",
        "  for website in websites :\n",
        "\n",
        "    website_cluster = them_cluster[them_cluster['Website'] == website]\n",
        "    plt.scatter(website_cluster['pos'],website_cluster['neg'],label=website)\n",
        "\n",
        "  plt.xlabel(\"Positivity\")\n",
        "  plt.ylabel(\"Negativity\")\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afURTeefL0K-"
      },
      "source": [
        "thematics = [[\"bank\",\"finance\",\"market\",\"stock\"],\n",
        "             [\"social\",\"health\"],\n",
        "             [\"unemployment\",\"employment\",\"work\"],\n",
        "             [\"immigration\",\"refugee\"],\n",
        "             [\"environment\",\"climate\",\"pollution\"]]\n",
        "\n",
        "for key_words in thematics :\n",
        "\n",
        "  them_cluster = thematic_sentiment(cleaned_res, key_words)\n",
        "  them_cluster = clust(them_cluster)\n",
        "  plot_them_cluster(them_cluster)\n",
        "  ind = them_cluster['Website'].value_counts().iloc[:3].index\n",
        "  plot_websites(them_cluster,ind)\n",
        "  ind = them_cluster['Website'].value_counts().iloc[:3].index\n",
        "  plot_websites(them_cluster,ind)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAg6FZIHJ4r3"
      },
      "source": [
        "You can also look at some well-known English-language newspapers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSffA-80HpxC"
      },
      "source": [
        "# newspapers = ['theguardian.com','thetimes.co.uk','nytimes.com','washingtonpost.com','economist.com','forbes.com']\n",
        "newspaper = ['theguardian.com']\n",
        "thematics = [[\"bank\",\"finance\",\"market\",\"stock\"],\n",
        "             [\"social\",\"health\"],\n",
        "             [\"unemployment\",\"employment\",\"work\"],\n",
        "             [\"immigration\",\"refugee\"],\n",
        "             [\"environment\",\"climate\",\"pollution\"]]\n",
        "\n",
        "for key_words in thematics :\n",
        "\n",
        "  them_cluster = thematic_sentiment(cleaned_res, key_words)\n",
        "  them_cluster = clust(them_cluster)\n",
        "  plot_them_cluster(them_cluster)\n",
        "  plot_websites(them_cluster,newspapers)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}